{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#!git clone https://github.com/gsethi2409/tf-pose-estimation.git"
      ],
      "metadata": {
        "id": "-onm8HpdtpTi"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd tf-pose-estimation\n",
        "# !pip install -r requirements.txt\n",
        "#restart colab"
      ],
      "metadata": {
        "id": "kXBaXUcjtvRt"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install swig"
      ],
      "metadata": {
        "id": "_oPTz95NuEQB",
        "outputId": "75e3bc0a-b880-4f57-d289-b9d91f97c167",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: swig in /usr/local/lib/python3.10/dist-packages (4.1.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd tf-pose-estimation/"
      ],
      "metadata": {
        "id": "K-3Pp5KVvR2j",
        "outputId": "6f319af2-dfae-418a-ab12-2da8b481adad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/tf-pose-estimation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd tf_pose/pafprocess"
      ],
      "metadata": {
        "id": "XGlVKMmQvW_J",
        "outputId": "181c2c53-a018-4b2a-9d6a-04fa1d3cb705",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/tf-pose-estimation/tf_pose/pafprocess\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!swig -python -c++ pafprocess.i && python3 setup.py build_ext --inplace\n"
      ],
      "metadata": {
        "id": "oXEvs96SuKxz",
        "outputId": "4977ff3a-3cdf-432d-f005-e38d3b302e71",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/tf-pose-estimation/tf_pose/pafprocess/setup.py:1: DeprecationWarning: The distutils package is deprecated and slated for removal in Python 3.12. Use setuptools or check PEP 632 for potential alternatives\n",
            "  from distutils.core import setup, Extension\n",
            "running build_ext\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/tf-pose-estimation/"
      ],
      "metadata": {
        "id": "RHxiu013uid6",
        "outputId": "e581e736-a578-40ad-de42-71b37dd65d64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/tf-pose-estimation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python"
      ],
      "metadata": {
        "id": "IWg0stoHuaWi",
        "outputId": "9a17b4bc-53b4-4bac-ad9f-49e32e4b6eba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.7.0.72)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.22.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"..\")"
      ],
      "metadata": {
        "id": "03j35v2Cwylj"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAMqFW1Yo-lG"
      },
      "source": [
        "# Chapter 7. 2D Body Pose Estimation\n",
        "\n",
        "[*Applied Machine Learning for Health and Fitness*](https://www.apress.com/9781484257715) by Kevin Ashley (Apress, 2020).\n",
        "\n",
        "[*Video Course*](http://ai-learning.vhx.tv) Need a deep dive? Watch my [*video course*](http://ai-learning.vhx.tv) that complements this book with additional examples and video-walkthroughs. \n",
        "\n",
        "[*Web Site*](http://activefitness.ai) for research and supplemental materials.\n",
        "\n",
        "\n",
        "> Nothing happens until something moves.\n",
        ">\n",
        "> Albert Einstein\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giOJyP3eo-lL"
      },
      "source": [
        "## Background\n",
        "\n",
        "Pose estimation is the problem of determining a human pose from an image or sequence of images. It also serves as a first step for mapping the pose in 3D (or 3D pose estimation) as discussed in the next chapter. \n",
        "\n",
        "![](https://github.com/Apress/Applied-Machine-Learning-for-Health-and-Fitness/blob/master/images/ch7/image1.png?raw=1)\n",
        "\n",
        "## Methods\n",
        "\n",
        "\n",
        "> For a successful technology, reality must take precedence over public relations, for nature cannot be fooled.\n",
        ">\n",
        "> Richard Feynman\n",
        "\n",
        "There're two approaches for 2D pose estimation that most models use: top-down and bottom-up. Bottom up approach is the fastest, it focuses on detecting keypoints first, then groups them into poses. The top-down approach detects people in the scene, and then applies a single person keypoints detection. \n",
        "\n",
        "Before diving into practical projects, let's to go over basic concepts of pose estimation: methods, datasets and data points, benchmarks and tools a sport data scientist can use.\n",
        "\n",
        "There're several deep learning models that work well for pose estimation; some notable examples include:\n",
        "\n",
        "-   Mask R-CNN, a two-stage method that works by generating proposals (areas of the image) in the first stage, and then creates masks or bounding boxes around areas.\n",
        "\n",
        "-   Stacked Hourglass Networks, use a method of consecutive top-down, bottom-up steps in conjunction with intermediate supervision for better capturing of spatial relationships associated with the body.\n",
        "\n",
        "-   Cascade Pyramid Networks (CPN) method deals with keypoints occlusion problems and tarets keypoint localization for the human body.\n",
        "\n",
        "-   Part Affinity Field method (used for example in OpenPose), a bottom-up system with real time characteristics.\n",
        "\n",
        "## Datasets\n",
        "\n",
        "For 2D pose estimation, several widely used datasets are available. Typically, pose estimation models are trained with some of these, providing good accuracy (3D datasets, such as Human3.6, DensePose, UP-3D and others will be covered in the next chapter):\n",
        "\n",
        "-   COCO (Common Objects in Context) dataset is based on 330K images and a quarter of a million people with keypoints. Microsoft and many other partners contributed to that dataset. The goal is placing the object recognition is the broader context of a scene understanding.\n",
        "\n",
        "-   MPII includes 25K images with over 40K people and 410 activities: images for this dataset are extracted from YouTube.\n",
        "\n",
        "-   LSP (Leeds Sports Pose Dataset) contains 2K pose annotated images taken from Flickr and includes 14 joints detected.\n",
        "\n",
        "-   FLIC (Frames Labeled in Cinema) has about 5K images from Hollywood movies and running a person detector on certain frames.\n",
        "\n",
        "**Note:** Some datasets available online may be automatically generated (for example from games) or other sources and may not contain precise athletic movements. We'll discuss recording custom datasets with labeled athletic data in Part IV: Machine Learning and Data.\n",
        "\n",
        "For pose estimation, we expect the data output as keypoints of human body. Body part mapping may be different, depending on what parts of the body you are interested in as a data scientist. For example, a typical keypoint data output using a COCO data set may include 18 body parts, as illustrated on the following figure:\n",
        "\n",
        "![COCO dataset keypoints](https://github.com/Apress/Applied-Machine-Learning-for-Health-and-Fitness/blob/master/images/ch7/image2.png?raw=1)\n",
        "\n",
        "\n",
        "Let's step back for a moment to understand how the concept of humanoid models, and body parts applies to sports movement analysis. From biomechanics, body is typically thought of as frontal, saggital and transverse planes. When coaches do movement analysis, they split body parts of a moving athlete into areas of interest.\n",
        "\n",
        "When I was an aspiring ski instructor, the first thing they taught me for movement analysis was looking at movements of the \"jacket\" (upper body) relative to \"pants\" (lower body). It turns out, this simple categorization of looking at the upper and lower body of a moving athlete provides enough first glance information for a ski instructor to detect a skill level. Most beginners would typically rotate upper body to turn, while a good skier would maintain a quiet upper body, and mostly apply lower body movements of ankles and knees for efficient, balanced turns.\n",
        "\n",
        "Similarly, to a human coach, pose estimation works by applying segmentation to detect keypoints. From Mask R-CNN to Part Affinity Field employed by OpenPose, neural nets follow the art of coaching in its quintessential form: from detecting areas of interest to segmentation and classification. In our practical projects further in this chapter, we will take that a step further to analyze real sport activities.\n",
        "\n",
        "## Benchmarks\n",
        "\n",
        "Benchmark scores across many methods have improved a lot over the last few years. That tells us that computer vision is making gigantic strides towards understanding of human motion and activities. In just one year, for pose estimation COCO mean average precision (mAP) metric has increased from 60% to 72.1%, another benchmark for MPII jumped from 80% to almost 90%! Even an Uber driver after an AI Symposium in Sunnyvale asked me how was Skynet doing, and how close we are to what's predicted in Terminator back in 1984. I'm not sure about Skynet, but Mobilenet, Posenet and many other convolution neural nets are doing very well!\n",
        "\n",
        "Most MPII winning methods are based on hourglass architecture with repeated bottom-up, top-down processing. Some COCO winning methods are based on CPN (cascaded pyramid network). Evaluation metrics frequently used in pose estimation are PCP percentage of correct parts (PCP), percentage of correct keypoints (PCK), percentage of detected joints (PDJ) and mean per joint position error (MPJPE).\n",
        "\n",
        "## Tools\n",
        "\n",
        "For a sport data scientist, there's a wide selection of tools available for 2D pose estimation. Practical projects in this chapter use different tools, to get you familiarized with a wide selection of technologies, rather than focusing on a single toolset. Most of these tools expose Python APIs, since the language became lingua franca of data science, so we'll continue this tradition and will use stock Python, Scikit-learn, OpenCV, PyTorch and Tensorflow for most of our projects for pose estimation.\n",
        "\n",
        "It's worth mentioning that most deep learning frameworks come packaged as CPU or GPU accelerated. I highly recommend spending a little bit of time to make sure that a GPU accelerated version of these libraries works in your environment! Deep learning using videos is resource intensive, and the difference between 0.3FPS and 30FPS is significant if you are a sport data scientist. Most Python packages have a GPU optimized package version. Oftentimes, investing in a dedicated AI optimized edge device makes a lot of sense.\n",
        "\n",
        "**Note:** Make sure you are using a GPU accelerated version of these tools or have a dedicated processing unit for deep learning (more on setting up the environment, see Part I).\n",
        "\n",
        "## Project 7-1: Pose Estimation - Is Your Stance Goofy or Regular?\n",
        "\n",
        "While I was working on computer vision problems in sports and fitness, I was introduced to the team working with Gabriel Medina Institute and Rip Curl on surfing research. California summer was getting closer, and being an aspiring surfer myself, I couldn't think of a better application for computer vision. I usually go surfing to Pacifica, my favorite beginner/intermediate break about half an hour away from my house. There, I started playing with a prototype of a smart surfing fin sensor and a custom-built computer camera with on-board computer vision. First things first, in surfing we begin by learning how to stand on the surfboard.\n",
        "\n",
        "![Using 2D pose estimation for surfing popup practice](https://github.com/Apress/Applied-Machine-Learning-for-Health-and-Fitness/blob/master/images/ch7/image3.jpeg?raw=1)\n",
        "\n",
        "\n",
        "All board sports have a notion of stance: be it surfboarding, snowboarding or skateboarding. The laterality of your brain impacts how footedness is chosen. Naturally, we stay with our left or right foot forward. It's very easy to test without the board by simply noticing what foot we lift first while stepping on the stair.\n",
        "\n",
        "![Stair test for determining your stance](https://github.com/Apress/Applied-Machine-Learning-for-Health-and-Fitness/blob/master/images/ch7/image4.png?raw=1)\n",
        "\n",
        "\"Goofy\" or \"regular\", means that we step with our right or left leg in front of the board. Wouldn't it be cool if our machine learning could help us recognizing the stance?\n",
        "\n",
        "**Note:** You need to install TensorFlow in your environment to run this project.\n",
        "\n",
        "\n",
        "![](https://github.com/Apress/Applied-Machine-Learning-for-Health-and-Fitness/blob/master/images/ch7/image5.png?raw=1)\n",
        "\n",
        "A goofy surfer (left) vs a regular surfer stance (right)\n",
        "\n",
        "To get started, we will use a single image of a surfer in the project folder and 2D pose estimation, to detect athlete's stance. We can also use continuous video stream from a web cam, to detect stance continuously in real time.\n",
        "\n",
        "![](https://github.com/Apress/Applied-Machine-Learning-for-Health-and-Fitness/blob/master/images/ch7/image6.png?raw=1)\n",
        "\n",
        "Figure 7-6 Keypoints of a surfer standing on the surfboard\n",
        "\n",
        "### Finding keypoints\n",
        "\n",
        "Let's define functions that help us determine stance. But first, we need to create functions that help locating points in the dataset, calculating Euclidian distance between points and calculating angles: "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install tf-pose"
      ],
      "metadata": {
        "id": "_LEh6qfAqOWB"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"..\")"
      ],
      "metadata": {
        "id": "IPW9QhZOyUA5"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Jzdyp_9So-lP"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import time\n",
        "import pandas as pd\n",
        "from tf_pose import common \n",
        "import numpy as np\n",
        "\n",
        "SK_BOARD_STANCE_GOOFY = \"goofy\"\n",
        "SK_BOARD_STANCE_REGULAR = \"regular\"\n",
        "\n",
        "def find_point(pose, p):\n",
        "    for point in pose:\n",
        "        try:\n",
        "            body_part = point.body_parts[p]\n",
        "            return (int(body_part.x * width + 0.5), int(body_part.y * height + 0.5))\n",
        "        except:\n",
        "            return (0,0)\n",
        "    return (0,0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkNTVE8Zo-lR"
      },
      "source": [
        "### Calculating Euclidian distance and angles\n",
        "\n",
        "Next, let's define functions to calculate Euclidian distance and angles:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "NSLjWdmzo-lS"
      },
      "outputs": [],
      "source": [
        "def euclidian(point1, point2):\n",
        "    return math.sqrt((point1[0]-point2[0])**2 + (point1[1]-point2[1])**2 )\n",
        "\n",
        "def angle_calc(p0, p1, p2 ):\n",
        "    try:\n",
        "        a = (p1[0]-p0[0])**2 + (p1[1]-p0[1])**2\n",
        "        b = (p1[0]-p2[0])**2 + (p1[1]-p2[1])**2\n",
        "        c = (p2[0]-p0[0])**2 + (p2[1]-p0[1])**2\n",
        "        angle = math.acos( (a+b-c) / math.sqrt(4*a*b) ) * 180/math.pi\n",
        "    except:\n",
        "        return 0\n",
        "    return int(angle)\n",
        "\n",
        "def center_of_mass(pose):\n",
        "    joints = np.zeros([common.CocoPart.Background.value, 2])\n",
        "\n",
        "    for p in pose:\n",
        "        for i in range(common.CocoPart.Background.value):\n",
        "            if i not in p.body_parts.keys():\n",
        "                continue\n",
        "            body_part = p.body_parts[i]\n",
        "            joints[i] = [body_part.x,body_part.y]\n",
        "    \n",
        "    return np.mean(joints, axis=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NnheRJFo-lT"
      },
      "source": [
        "### Determining surfer stance\n",
        "\n",
        "Finally, using methods defined earlier to calculate angles, we can get to stance estimation:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "kXK54GwUo-lU"
      },
      "outputs": [],
      "source": [
        "def surfing_check_goofy_stance(is_mirror, a, b):\n",
        "    '''\n",
        "        is_mirror shows if the camera is mirrored\n",
        "        a neck L angle\n",
        "        b neck R angle\n",
        "    '''\n",
        "    if not is_mirror:\n",
        "        if a > b:\n",
        "            return SK_BOARD_STANCE_GOOFY\n",
        "        return SK_BOARD_STANCE_REGULAR\n",
        "    else:\n",
        "        if a > b:\n",
        "            return SK_BOARD_STANCE_REGULAR\n",
        "        return SK_BOARD_STANCE_GOOFY\n",
        "    \n",
        "def detect_stance(is_mirror, pose):\n",
        "    l = angle_calc(find_point(pose,0), find_point(pose,1), find_point(pose,11))\n",
        "    r = angle_calc(find_point(pose,0), find_point(pose,1), find_point(pose,8))\n",
        "    \n",
        "    return surfing_check_goofy_stance(is_mirror, l, r)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rg0ixOt_o-lV"
      },
      "source": [
        "Now, it's time to import TensorFlow and supporting modules:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ZKVMmRkPo-lW"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import time\n",
        "import os\n",
        "\n",
        "from tf_pose  import common\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tf_pose.estimator import TfPoseEstimator\n",
        "from tf_pose.networks import get_graph_path, model_wh\n",
        "import math\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "BOARD_STANCE_GOOFY = \"goofy\"\n",
        "BOARD_STANCE_REGULAR = \"regular\"\n",
        "\n",
        "model = \"cmu\"\n",
        "image_file = \"media/surfer_656x368.jpg\"\n",
        "experiments_dir = \"_experiments/\"\n",
        "\n",
        "def draw_str(dst, s, color, scale):\n",
        "    \n",
        "    (x, y) = (10,20)\n",
        "\n",
        "    if (color[0]+color[1]+color[2]==255*3):\n",
        "        cv2.putText(dst, s, (x+1, y+1), cv2.FONT_HERSHEY_SIMPLEX, scale, (0, 0, 0))\n",
        "    else:\n",
        "        cv2.putText(dst, s, (x+1, y+1), cv2.FONT_HERSHEY_SIMPLEX, scale, color)\n",
        "    #cv2.line    \n",
        "    cv2.putText(dst, s, (x, y), cv2.FONT_HERSHEY_SIMPLEX, scale, (255, 255, 255))\n",
        "\n",
        "def init_experiments_dir(experiments_dir = \"_experiments/\"):\n",
        "    if not os.path.exists(experiments_dir):\n",
        "        os.makedirs(experiments_dir)\n",
        "\n",
        "    experiment_dir = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "    out_dir = experiments_dir + experiment_dir\n",
        "    if not os.path.exists(out_dir):\n",
        "        os.makedirs(out_dir)\n",
        "\n",
        "    output_json = out_dir + \"/json/\"\n",
        "    if not os.path.exists(output_json):\n",
        "        os.makedirs(output_json)\n",
        "    return out_dir, output_json\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRT6mImNo-lX"
      },
      "source": [
        "\n",
        "Initialize pose estimator and read the image:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Lk_CpBF-o-lY",
        "outputId": "43a99929-2ffc-4637-c079-f26dc426cff1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "error",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-f2fdb10835cf>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mout_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_experiments_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiments_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_imgfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31merror\u001b[0m: OpenCV(4.7.0) /io/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
          ]
        }
      ],
      "source": [
        "# initialize and load images\n",
        "out_dir, output_json = init_experiments_dir(experiments_dir)\n",
        "image = common.read_imgfile(image_file, None, None)\n",
        "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWvTvg11o-lZ"
      },
      "outputs": [],
      "source": [
        "# load estimator\n",
        "h, w, channels = image.shape\n",
        "\n",
        "if w == 0 or h == 0:\n",
        "    e = TfPoseEstimator(get_graph_path('cmu'), target_size=(432, 368))\n",
        "else:\n",
        "    e = TfPoseEstimator(get_graph_path('cmu'), target_size=(w, h))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orkKt9f-o-la"
      },
      "source": [
        "Start inference and estimate the stance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3jHsXeYo-la"
      },
      "outputs": [],
      "source": [
        "print('inference started...')\n",
        "t = time.time()\n",
        "pose = e.inference(image, resize_to_default=(w > 0 and h > 0), upsample_size=4.0)\n",
        "\n",
        "print(\"STANCE \"+ detect_stance(True, pose))\n",
        "\n",
        "image = TfPoseEstimator.draw_humans(image, pose, imgcopy=False, output_json_dir=out_dir )    \n",
        "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGP9sQxio-la"
      },
      "source": [
        "How it works? This example uses Tensorflow and OpenPose for pose estimation. The method we use to determine the stance is by estimating the angle between head and hips. A greater angle between left hip and the head means that the athlete is standing in goofy stance, when the camera is not using mirrored mode. Otherwise, we assume that the stance is regular. It's important to notice that in selfies mode, most cameras mirror the image, so to determine stance correctly, we need to pass an argument to the method to tell it whether camera or image is mirrored. In detect\\_stance function we calculate angles between the keypoints, then check stance condition in surfing\\_check\\_goofy\\_stance function.\n",
        "\n",
        "To run stance estimation using live web cam real-time, check utils/dtw/pose\\_estimation\\_webcam.py. That script will use your web cam, instead of the image to detect stance, similarly we can use a video file as an input."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5qB_pj5o-la"
      },
      "source": [
        "\n",
        "## Activity Recognition\n",
        "\n",
        "> Fall in love with some activity, and do it!\n",
        ">\n",
        "> Richard Feynman\n",
        "\n",
        "In the stance detection project we looked at a very simple problem, by focusing on an individual image or frames. This easy approach quickly gets results for estimating a static pose, but since most of the sports are recorded on video, we can improve precision and recognize entire activities, as sequences of events, rather than looking at static poses.\n",
        "\n",
        "![](https://github.com/Apress/Applied-Machine-Learning-for-Health-and-Fitness/blob/master/images/ch7/image8.png?raw=1)\n",
        "\n",
        "Recognizing activity using rules is useful to understand joint mechanics, but we can also use deep learning methods for activity recognition. Methods for temporal classifications have evolved: Dynamic Time Warping (DTW), Hidden Markov Models (HMM), Temporal Convolutional Networks (TCNs), recurrent neural networks with LSTM neurons, and others to analyze human movements.\n",
        "\n",
        "## Dynamic Time Warping (DTW)\n",
        "\n",
        "Dynamic Time Warping helps finding similarities in temporal sequences. For a coach, this is the Holy Grail of sports science! Here's the essence of coaches' work: run after run, athletes perform activities that need to be analyzed, compared with best performing athletes (models) and athlete's own results, then the feedback provided to the athlete that summarizes areas of improvements. Applying methods that take into account time, such as DTW, to each set of activities can help us train the model to detect activities as they happen dynamically, and from the coach prospective: detect issues with the movements! For a practical sports scientist looking to analyze athlete movements, this sounds like a great solution, however it's worth mentioning that like all methods, DTW needs to be applied constrained, and using sets of data that imply similarity.\n",
        "\n",
        "In our quest for AI coach, we'll build a quick pipeline to detect some basic activities from temporal sequences and provide a feedback. But first, some basic ideas behind this method.\n",
        "\n",
        "## Project: Appying DTW to pose estimation keypoints\n",
        "\n",
        "### Getting project data\n",
        "\n",
        "From project 7-1 you already know how to get a 2D body pose estimation, using either a static image, video or a webcam. Using tools, such an OpenPose, it's easy to save keypoints into a JSON file, for each frame and each person detected. Conveniently, OpenPose provides methods for saving keypoints out of the box, by using ---write\\_json flag:\n",
        "\n",
        "```bash\n",
        "$openpose.bin --model_pose COCO --video media/video.avi --write_json media/json\n",
        "```\n",
        "\n",
        "This instructs OpenPose to process our video file, using COCO model, and save keypoints in JSON files. The output is a single JSON file per frame, similar to the following, where pose\\_keypoints\\_2 represents an array of 18 keypoint parts locations, followed by confidence value, e.g. \\[x1,y1,c1,x2,y2,c2,...\\]:\n",
        "\n",
        "```json\n",
        "{\n",
        "    \"version\": 1.3,\n",
        "    \"people\": [\n",
        "    {\n",
        "        \"person_id\": [-1],\n",
        "        \"pose_keypoints_2d\": [\n",
        "            413.834,\n",
        "            102.427,\n",
        "            0.896774,\n",
        "```\n",
        "\n",
        "...\n",
        "\n",
        "Since we requested COCO pose model, we'll get 18 keypoints. Other options are 25 keypoints (COCO+feet) and MPI (15 keypoints) which may be useful depending on whether we need to get faster performance or mode data for human pose analysis. We are interested in x and y positions, so we'll reshape the array (MODEL\\_KEYPOINTS=18 for COCO pose model):\n",
        "\n",
        "pose\\_keypoints\\_2d.reshape((MODEL\\_KEYPOINTS, 3))\n",
        "\n",
        "For an example on how to load keypoints from 2D pose estimation into a NumPy array, look at utils\\_load\\_keypoints notebook.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0j1djP1o-lb"
      },
      "source": [
        "### Basic idea\n",
        "\n",
        "We will run DTW on the set of keypoints detected earlier with 2D Pose Estimation. The code to parse and load OpenPose keypoints is in pose module:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBETekI6o-lb"
      },
      "outputs": [],
      "source": [
        "from utils.keypoints import pose\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.signal import medfilt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neighbors import DistanceMetric\n",
        "from sklearn.metrics import classification_report\n",
        "%matplotlib inline\n",
        "\n",
        "def load_features(names):\n",
        "    output = [] \n",
        "    for filename in names:\n",
        "        ps = pose.PoseSequence.load('data/dtw/sample/'+filename)\n",
        "        poses = ps.poses\n",
        "\n",
        "        right_present = [1 for pose in poses \n",
        "                if pose.rshoulder.exists and pose.relbow.exists and pose.rwrist.exists]\n",
        "        left_present = [1 for pose in poses\n",
        "                if pose.lshoulder.exists and pose.lelbow.exists and pose.lwrist.exists]\n",
        "        right_count = sum(right_present)\n",
        "        left_count = sum(left_present)\n",
        "        side = 'right' if right_count > left_count else 'left'\n",
        "\n",
        "        if side == 'right':\n",
        "            joints = [(pose.rshoulder, pose.relbow, pose.rwrist, pose.rhip, pose.neck) for pose in poses]\n",
        "        else:\n",
        "            joints = [(pose.lshoulder, pose.lelbow, pose.lwrist, pose.lhip, pose.neck) for pose in poses]\n",
        "\n",
        "        # filter out data points where a part does not exist\n",
        "        joints = [joint for joint in joints if all(part.exists for part in joint)]\n",
        "        \n",
        "        upper_arm_vecs = np.array([(joint[0].x - joint[1].x, joint[0].y - joint[1].y) for joint in joints])\n",
        "        forearm_vecs = np.array([(joint[2].x - joint[1].x, joint[2].y - joint[1].y) for joint in joints])\n",
        "        \n",
        "        upper_arm_vecs = upper_arm_vecs / np.expand_dims(np.linalg.norm(upper_arm_vecs, axis=1), axis=1)\n",
        "        forearm_vecs = forearm_vecs / np.expand_dims(np.linalg.norm(forearm_vecs, axis=1), axis=1)\n",
        "        \n",
        "        upper_arm_forearm_angle = np.degrees(np.arccos(np.clip(np.sum(np.multiply(upper_arm_vecs, forearm_vecs), axis=1), -1.0, 1.0)))\n",
        "        upper_arm_forearm_angle_filtered = medfilt(medfilt(upper_arm_forearm_angle, 5), 5)\n",
        "\n",
        "        output.append(upper_arm_forearm_angle_filtered.tolist())\n",
        "    return output\n",
        "\n",
        "   \n",
        "def path_cost(x, y, accumulated_cost, distances):\n",
        "    path = [[len(x)-1, len(y)-1]]\n",
        "    cost = 0\n",
        "    i = len(x)-1\n",
        "    j = len(y)-1\n",
        "    while i>0 and j>0:\n",
        "        if i==0:\n",
        "            j = j - 1\n",
        "        elif j==0:\n",
        "            i = i - 1\n",
        "        else:\n",
        "            if accumulated_cost[i-1, j] == min(accumulated_cost[i-1, j-1], accumulated_cost[i-1, j], accumulated_cost[i, j-1]):\n",
        "                i = i - 1\n",
        "            elif accumulated_cost[i, j-1] == min(accumulated_cost[i-1, j-1], accumulated_cost[i-1, j], accumulated_cost[i, j-1]):\n",
        "                j = j-1\n",
        "            else:\n",
        "                i = i - 1\n",
        "                j= j- 1\n",
        "        path.append([i, j])\n",
        "    path.append([0,0])\n",
        "    for [x,y] in path:\n",
        "        cost = cost +distances[x,y]\n",
        "    return path, cost    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbpznCKOo-lb"
      },
      "outputs": [],
      "source": [
        "data = load_features(['x.npy','y.npy'])\n",
        "x = data[0]\n",
        "y = data[1]\n",
        "plt.plot(x, 'g', label='X')\n",
        "plt.plot(y, 'r', label='Y')\n",
        "plt.legend();\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCrTDIQpo-lb"
      },
      "source": [
        "It looks like there's similarity in this data, but the speed and duration are different. A naïve approach to find similarities is finding Euclidian distances, by simply looking at points that can be matched together:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kASKUBA1o-lc"
      },
      "outputs": [],
      "source": [
        "distances = np.zeros((len(x), len(y)))\n",
        "distances.shape\n",
        "for i in range(len(x)):\n",
        "    for j in range(len(y)):\n",
        "        distances[i,j] = (x[i]-y[j])**2 \n",
        "distances"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbcYJSORo-lc"
      },
      "source": [
        "Visualizing the cost matrix for distances shows that diagonal elements have lower distance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kB5UemMIo-lc"
      },
      "outputs": [],
      "source": [
        "def distance_cost_plot(distances):\n",
        "    im = plt.imshow(distances, interpolation='nearest', cmap='Blues') \n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.xlabel(\"X\")\n",
        "    plt.ylabel(\"Y\")\n",
        "    plt.grid()\n",
        "    plt.colorbar()\n",
        "\n",
        "distance_cost_plot(distances)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZfxt0kQo-lc"
      },
      "source": [
        "The optimal warping path between X and Y is defined as warping path having minimal total cost among all possible paths. The DTW distance is defined as the total cost of p:\n",
        "\n",
        "$$\\text{DTW}\\left( X,Y \\right) ≔ c_{p}\\left( X,Y \\right) = \\min_{}{\\left\\{ c_{p}\\left( X,Y \\right)\\  \\right|\\text{\\p\\ is\\ an\\ }\\left( N,M \\right)\\ warping\\ path\\}}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2TPVEaPo-ld"
      },
      "outputs": [],
      "source": [
        "accumulated_cost = np.zeros((len(x), len(y)))\n",
        "accumulated_cost[0,0] = distances[0,0]\n",
        "for i in range(1, len(x)):\n",
        "    accumulated_cost[i,0] = distances[i,0] + accumulated_cost[i-1,i] \n",
        "for i in range(1, len(x)):\n",
        "    for j in range(1, len(y)):\n",
        "        accumulated_cost[i, j] = min(accumulated_cost[i-1, j-1], accumulated_cost[i-1, j], accumulated_cost[i, j-1]) + distances[i, j]\n",
        "        \n",
        "distance_cost_plot(accumulated_cost)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4bhEZceo-le"
      },
      "outputs": [],
      "source": [
        "path = [[len(x)-1, len(y)-1]]\n",
        "i = len(x)-1\n",
        "j = len(y)-1\n",
        "while i>0 and j>0:\n",
        "    if i==0:\n",
        "        j = j - 1\n",
        "    elif j==0:\n",
        "        i = i - 1\n",
        "    else:\n",
        "        if accumulated_cost[i-1, j] == min(accumulated_cost[i-1, j-1], accumulated_cost[i-1, j], accumulated_cost[i, j-1]):\n",
        "            i = i - 1\n",
        "        elif accumulated_cost[i, j-1] == min(accumulated_cost[i-1, j-1], accumulated_cost[i-1, j], accumulated_cost[i, j-1]):\n",
        "            j = j-1\n",
        "        else:\n",
        "            i = i - 1\n",
        "            j= j- 1\n",
        "    path.append([i, j])\n",
        "path.append([0,0])\n",
        "\n",
        "path_x = [point[0] for point in path]\n",
        "path_y = [point[1] for point in path]\n",
        "\n",
        "distance_cost_plot(accumulated_cost)\n",
        "plt.plot(path_x, path_y)\n",
        "plt.show()\n",
        "\n",
        "path, cost = path_cost(x, y, accumulated_cost, distances)\n",
        "print(cost)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpRb5doTo-le"
      },
      "source": [
        "We can check our calculations by using a stock Python library, such as FastDTW to test the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRYl9S7Ho-le"
      },
      "outputs": [],
      "source": [
        "from scipy.spatial.distance import euclidean\n",
        "from fastdtw import fastdtw\n",
        "distance, path = fastdtw(x, y, dist=euclidean)\n",
        "print(distance)\n",
        "\n",
        "path_x = [point[0] for point in path]\n",
        "path_y = [point[1] for point in path]\n",
        "plt.plot(path_x, path_y)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QoNzTndio-le"
      },
      "outputs": [],
      "source": [
        "plt.plot(x, 'crimson' ,label='x')\n",
        "plt.plot(y, 'orange',label = 'y')\n",
        "plt.legend();\n",
        "paths = path_cost(x, y, accumulated_cost, distances)[0]\n",
        "for [map_x, map_y] in paths:\n",
        "    plt.plot([map_x, map_y], [x[map_x], y[map_y]], 'maroon')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCCk7y_zo-le"
      },
      "source": [
        "Which gives us similar results: both paths look very similar. Dynamic Time Warping is a useful technique for analyzing temporal sequences and finding similarities in sequences.\n",
        "\n",
        "## Detecting skill level\n",
        "\n",
        "Detecting a skill level in sports is one of the most important problems for a coach: from the moment we see an athlete, neural networks in our brain attempt to classify movements of that athlete, analyzing how good of an athlete that is. Parhaps, this natural function of our brains is related to our basic survival instincts: this classification is trained to be super-fast, in an attempt to protect us from unfriendly or dangerous acts. Since ancient times, we had to quickly estimate the level of danger; with the evolution of human society, we instinctively look for individuals who we can learn from, by classifying or comparing our own skill to theirs.\n",
        "\n",
        "Most modern sports developed comprehensive systems that define levels. For example, Professional Ski and Snowboard Instructors of America have defined 10 skill levels that help classify skiers and recommend course of training. Generally, most sports classify athletes' skill as beginner, intermediate and advanced (experts). In our next project we will use pose estimation to tell an advanced athlete from a beginner.\n",
        "\n",
        "### Using pose estimation for skill level detection\n",
        "\n",
        "For this project we'll dive into skiing, a highly dynamic sport. Even if you are not a skier, by using data from pose estimation, you'll be able to use methods described earlier to tell an advanced level athlete from a beginner.\n",
        "\n",
        "![](https://github.com/Apress/Applied-Machine-Learning-for-Health-and-Fitness/blob/master/images/ch7/image13.jpeg?raw=1)\n",
        "\n",
        "Figure 7-13 Athlete with advanced skill level: body pose keypoints\n",
        "\n",
        "Let's look at a beginner level skier, in this case a kid skiing wedge type of turn in a typical beginner stance. Apart from balance and center of mass being impacted by body proportions (for kids, their head is bigger relative to the body), a professional instructor can immediately identify a number of elements typical for beginners. Let's see if we can use machine learning methods to help us identify those and predict skill levels of athletes.\n",
        "\n",
        "![](https://github.com/Apress/Applied-Machine-Learning-for-Health-and-Fitness/blob/master/images/ch7/image14.png?raw=1)\n",
        "\n",
        "Figure 7-14 Keypoints and pose of a beginner skier\n",
        "\n",
        "Looking at the body position, notice that the center of mass (CM) of the skier is far back, instead of being on top of the center of the skis. Angle β between torso and hips tends to be high, the kid is \"sitting back\", which is very typical for kids skiing but is also characteristic for beginner adult skiers.\n",
        "\n",
        "![](https://github.com/Apress/Applied-Machine-Learning-for-Health-and-Fitness/blob/master/images/ch7/image15.png?raw=1)\n",
        "\n",
        "Figure 7-15 Keypoints visualization of a beginner skier\n",
        "\n",
        "Note also the neck angle α: beginners tend to look at the ski tips, instead of looking straight, about 3 turns ahead, as advanced skiers do, the neck is also extended forward to compensate for the backward body position. Many beginners tend to use upper body to turn, instead of using ankles and knees. On the video it's clear that what's happening here is hips, ankles and knees stay static, locked in so-called A-frame pose, while the upper body swings, providing a momentum for turn.\n",
        "\n",
        "![](https://github.com/Apress/Applied-Machine-Learning-for-Health-and-Fitness/blob/master/images/ch7/image16.png?raw=1)\n",
        "\n",
        "Figure 7-16 Beginner skier visualization sequence\n",
        "\n",
        "We will use an advanced skier dataset and a dataset from the beginner skier to compare and predict the skill level based on differences in body joints relations. First, we get neck and torso vectors from body parts:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ri4XW22Qo-lf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import glob\n",
        "import utils\n",
        "\n",
        "from utils.keypoints import pose, utils\n",
        "from pprint import pprint\n",
        "from scipy.signal import medfilt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neighbors import DistanceMetric\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "files = utils.files_in_order('data/dtw/skiing/experiments')\n",
        "#print(files)\n",
        "X_train_names, X_test_names = train_test_split(files, test_size=0.4, random_state=42)\n",
        "y_train = utils.get_labels_by_level(X_train_names)\n",
        "y_test = utils.get_labels_by_level(X_test_names)\n",
        "pprint(X_train_names)\n",
        "pprint(y_train)\n",
        "pprint(X_test_names)\n",
        "pprint(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2FDcp21o-lf"
      },
      "outputs": [],
      "source": [
        "def load_features(names):\n",
        "    output1 = [] # List of upper arm torso angles\n",
        "    output2 = [] # List of forearm upper arm angles\n",
        "    for filename in names:\n",
        "        ps = pose.PoseSequence.load('data/dtw/skiing/experiments/'+filename)\n",
        "        poses = ps.poses\n",
        "\n",
        "        dominant_side = utils.get_side(poses)\n",
        "    \n",
        "        if dominant_side == 'right':\n",
        "            joints = utils.get_joints([(pose.nose, pose.neck, pose.rhip) for pose in poses])\n",
        "        else:\n",
        "            joints = utils.get_joints([(pose.nose, pose.neck, pose.lhip) for pose in poses])\n",
        "\n",
        "        # looking down at skis\n",
        "        neck = utils.get_normalized_joint_vector(joints, 0, 1)\n",
        "        torso = utils.get_normalized_joint_vector(joints, 1, 2)\n",
        "        angles = utils.get_angle(torso, neck)\n",
        "        angles_filtered = medfilt(medfilt(angles, 5), 5)\n",
        "        print(\"Max torso/neck angle: \", np.max(angles))\n",
        "        utils.chart(angles, filename, 'Torso/Neck angle', 'blue')\n",
        "\n",
        "        joints = utils.get_joints([(pose.rhip, pose.lhip, pose.rknee, pose.lknee, pose.rankle, pose.lankle) for pose in poses])\n",
        "\n",
        "        rhip = utils.get_normalized_joint_vector(joints, 0, 2)\n",
        "        lhip = utils.get_normalized_joint_vector(joints, 1, 3)\n",
        "        rankle = utils.get_normalized_joint_vector(joints, 2, 4)\n",
        "        lankle = utils.get_normalized_joint_vector(joints, 3, 5)\n",
        "\n",
        "        # calculate angles\n",
        "        rknee_angle = utils.get_angle(rhip, rankle)\n",
        "        lknee_angle = utils.get_angle(lhip, lankle)\n",
        "        \n",
        "        rknee_angle_filtered = medfilt(medfilt(rknee_angle, 5), 5)\n",
        "        lknee_angle_filtered = medfilt(medfilt(lknee_angle, 5), 5)\n",
        "        \n",
        "        output1.append(angles_filtered.tolist())\n",
        "        output2.append(lknee_angle_filtered.tolist())\n",
        "    return output1, output2\n",
        "\n",
        "X_train_1, X_train_2 = load_features(X_train_names)\n",
        "X_test_1, X_test_2 = load_features(X_test_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5vlDMhxo-lf"
      },
      "source": [
        "Result clearly shows that the beginner skier has upper body rotating and swinging a lot more than an advanced athlete. This result tells us what a ski instructor could explain in a few seconds by observing both skiers: quiet upper body of a more advanced athlete, and turns initiated from ankles and knees, vs swinging the body in case of a beginner.\n",
        "\n",
        "## Multi-person pose estimations\n",
        "\n",
        "Many sports involve a multi-player action, so it is important to detect groups of people. Fortunately, most methods we discussed above for 2D and 3D body pose estimations apply for detection in the group. One of the most important aspects of group detection for sports scenarios is performance (earlier we discussed top-down and bottom-up approaches to pose estimation).\n",
        "\n",
        "![](https://github.com/Apress/Applied-Machine-Learning-for-Health-and-Fitness/blob/master/images/ch7/image19.jpeg?raw=1)\n",
        "\n",
        "Fortunately for sport data scientists, 2D pose estimation works really well in multi-person scenario. In the following project, we will use a simple people count, detected from pose estimation.\n",
        "\n",
        "### Project: Counting people with multi-person pose estimation\n",
        "\n",
        "From previous examples, we already initialized TensorFlow, let's load an image with multiple subjects:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vxDdJJJ4o-lf"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import time\n",
        "import os\n",
        "\n",
        "from tf_pose import common\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tf_pose.estimator import TfPoseEstimator\n",
        "from tf_pose.networks import get_graph_path, model_wh\n",
        "import math\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "model = \"cmu\"\n",
        "image_file = \"media/multiple_skiers_656x368.png\"\n",
        "image = common.read_imgfile(image_file, None, None)\n",
        "  \n",
        "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10J1YzxZo-lg"
      },
      "source": [
        "Now ley's simply use len(people) to get the size of the array from the inference:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6V8iA9K3o-lg"
      },
      "outputs": [],
      "source": [
        "h, w, channels = image.shape\n",
        "\n",
        "print(\"Loading TensorFlow\")\n",
        "if w == 0 or h == 0:\n",
        "    e = TfPoseEstimator(get_graph_path('cmu'), target_size=(432, 368))\n",
        "else:\n",
        "    e = TfPoseEstimator(get_graph_path('cmu'), target_size=(w, h))\n",
        "    \n",
        "    \n",
        "experiments_dir = \"_experiments\"\n",
        "if not os.path.exists(experiments_dir):\n",
        "    os.makedirs(experiments_dir)\n",
        "\n",
        "experiment_dir = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "out_dir = experiments_dir + experiment_dir\n",
        "if not os.path.exists(out_dir):\n",
        "    os.makedirs(out_dir)\n",
        "\n",
        "output_json = out_dir + \"/json/\"\n",
        "if not os.path.exists(output_json):\n",
        "    os.makedirs(output_json)\n",
        "\n",
        "print('inference started...')\n",
        "t = time.time()\n",
        "people = e.inference(image, resize_to_default=(w > 0 and h > 0), upsample_size=4.0)\n",
        "count_people = len(people)\n",
        "print(\"NUMBER OF PEOPLE \", count_people)\n",
        "image = TfPoseEstimator.draw_humans(image, people, imgcopy=False, output_json_dir=output_json ) \n",
        "cv2.putText(image,\n",
        "            \"People Count: %d\" % (count_people),\n",
        "            (10, 50),  cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
        "            (255, 255, 255), 2)\n",
        "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cE2V1h-o-lg"
      },
      "source": [
        "Once we have a group of people detected, we could apply prediction methods, such as LSTM or use techniques such as dynamic time warping to predict the path of individual within a group.\n",
        "\n",
        "## Dealing with loss and occlusion\n",
        "\n",
        "Occasionally, models may exclude points that are completely hidden from the observer (keypoint probability drops to 0.0 in our data). As a practical data scientist, you can deal with the problem in many ways. Depending on the activity: a data scientist may have an option of installing additional cameras with additional view angles to reduce the number of occluded parts. This method is called multi-view video triangulation. Depth sensors, such as LIDARs may provide additional information, as in the case mentioned earlier in this book with AI judging system in gymnastics. If the activity you're studying allows the use of additional sensors (such as IMUs) and the activity allows placing sensors on athlete's body: inertial movement sensors don't suffer from optical occlusion, and are discussed in other parts of this book, dedicated to sensors and hybrid methods.\n",
        "\n",
        "One simple method, based on the datasets we are dealing with, is including only the frames with joints present across all frames. This approach is used in get\\_joints method defined in utils.py in projects for this chapter:\n",
        "\n",
        "```python\n",
        "joints = [joint for joint in poses if all(part.exists for part in joint)]\n",
        "```\n",
        "\n",
        "Despite occlusion, pose estimation remains a very robust, practical and near real-time method for movement analysis in sports.\n",
        "\n",
        "Summary\n",
        "=======\n",
        "\n",
        "In this chapter we looked at 2D body pose estimation applications for sports. 2D body pose estimations methods and models have reached maturity and precision. We demonstrated in the projects in this chapter, with highly dynamic sports, including surfing and skiing. Pose estimation can be applied in sports movement analysis, if you are willing to accept occasional instances of occlusion and exclude frames with loss of keypoints. In the next chapter we extend techniques we used in 2D body pose estimation to three-dimensional models.\n",
        "\n",
        "## Reference\n",
        "\n",
        "[*Video Course*](http://ai-learning.vhx.tv) Need a deep dive? Watch my [*video course*](http://ai-learning.vhx.tv) that complements this book with additional examples and video-walkthroughs. \n",
        "\n",
        "[*Web Site*](http://activefitness.ai) for research and supplemental materials.\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYJxGdc-o-lg"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}